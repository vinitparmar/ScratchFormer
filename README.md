# ğŸ§© ScratchFormer

**ScratchFormer** is a Transformer neural network built **entirely from scratch** using **NumPy**.  
This project helps you deeply understand how modern architectures like GPT and BERT actually work â€” by implementing every part manually, from attention to embeddings.

---

## ğŸš€ Features
- âœ… Pure **NumPy** implementation (no deep learning frameworks)
- ğŸ§  Full **Encoderâ€“Decoder Transformer** architecture
- ğŸ” Implements:
  - Scaled Dot-Product & Multi-Head Attention  
  - Positional Encoding (sinusoidal)  
  - Feed Forward Networks (GELU activation)  
  - Layer Normalization & Residual Connections
- ğŸ§© Modular and easy to extend
- ğŸ’¡ Educational, readable, and well-documented
- ğŸ§ª Includes toy training example (copy / translation task)

---
HASE 1 â€” Foundations

ğŸ“… Time: 1â€“2 days

1. Set up repository

Folder structure (scratchformer/, examples/, notebooks/)

Add README + requirements

Create empty module files

2. Write utility math functions

softmax

stable softmax (numerically safe)

create masks (padding + causal)

matrix operations (optional helpers)

ğŸ“Œ Goal: be comfortable with matrix shapes (batch, seq, dim).

PHASE 2 â€” Core Components (Building Blocks)

ğŸ“… Time: 3â€“5 days

We code every block manually.

3. Token Embedding

lookup matrix vocab_size Ã— d_model

convert token IDs â†’ vectors

4. Positional Encoding

sinusoidal positional encoding (NumPy)

add to embeddings

5. Scaled Dot-Product Attention

compute Q, K, V

formula:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^{T}}{\sqrt{d_k}}\right)V
$$

test with small input

6. Multi-Head Attention

linear projection into heads

split â†’ attention â†’ concat

output projection

ensure shapes match exactly

7. Feed Forward Network

Dense â†’ GELU â†’ Dense

per-position (works on each token independently)

8. Layer Normalization

implement from scratch:

mean

variance

normalize

gamma, beta parameters

ğŸ“Œ Goal: Each block should be testable alone with a small script.

PHASE 3 â€” Encoder-Decoder Architecture

ğŸ“… Time: 3â€“5 days

9. Encoder Layer

multi-head self-attention

residual + layernorm

feed-forward

residual + layernorm

test with random tokens

10. Decoder Layer

masked self-attention

encoderâ€“decoder attention

feed-forward

residuals + norms

11. Encoder Stack

stack N layers in a loop

12. Decoder Stack

stack N layers in a loop

ğŸ“Œ Goal: Build full working encoder & decoder.

PHASE 4 â€” Full Transformer Model

ğŸ“… Time: 3â€“4 days

13. Combine encoder + decoder

input embeddings

positional encodings

encoder output â†’ decoder input

final linear layer projecting to vocab size

14. Forward pass

accept:

src_tokens

tgt_tokens

masks

output logits

15. Greedy decoding

autoregressive decoding

feed previous tokens into decoder

generate sequences

ğŸ“Œ Goal: Model can run a forward pass and generate output.

PHASE 5 â€” Training (Toy Examples)

ğŸ“… Time: 3â€“6 days

16. Build simple cross-entropy loss

compute average loss ignoring padding tokens

17. Create toy dataset

Examples:

Copy task (Y = X)

Reverse task (Y = reversed(X))

Shift-by-one task

Tiny translation mapping

18. Training loop

forward pass

compute loss

backprop through NumPy (optional)

OR partially use PyTorch autograd

update weights manually (SGD or Adam)

ğŸ“Œ Goal: Loss should go down after 5â€“20 epochs.

PHASE 6 â€” Enhancements (Optional but Powerful)

ğŸ“… Time: 1â€“2 weeks

19. Port model to PyTorch

same architecture, easier training

GPUs + autograd

20. Visualize Attention

plot attention matrices

use matplotlib or seaborn

21. Add support for larger configs

more layers

more attention heads

22. Train on a real dataset

small translation dataset (IWSLT)

character-level modeling

mini GPT

PHASE 7 â€” Release & Document

ğŸ“… Time: 1â€“2 days

23. Clean codebase

comments

modular structure

remove unused code

24. Final README updates

diagrams (like the one generated)

architecture explanation

code examples

formulas

---

## ğŸ—ï¸ Project Structure

  ScratchFormer/
  â”‚
  â”œâ”€â”€ README.md
  â”œâ”€â”€ requirements.txt
  â”‚
  â”œâ”€â”€ scratchformer/
  â”‚ â”œâ”€â”€ init.py
  â”‚ â””â”€â”€ transformer_from_scratch_numpy.py # Main transformer implementation
  â”‚
  â”œâ”€â”€ examples/
  â”‚ â””â”€â”€ copy_task_demo.py # Simple demo/training example
  â”‚
  â””â”€â”€ notebooks/
  â””â”€â”€ transformer_from_scratch.ipynb # (Optional) Jupyter notebook for explanation


## âš™ï¸ Installation & Setup

1. **Clone this repository**
   ```bash
   git clone https://github.com/<your-username>/ScratchFormer.git
   cd ScratchFormer

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt

3. **Run the demo**
   ```bash
   python examples/copy_task_demo.py

---
## Block Diagram

<img width="1024" height="1024" alt="image" src="https://github.com/user-attachments/assets/574f9775-ec8f-4376-a7f2-862f22190a54" />

ğŸ§± 1. Embedding & Positional Encoding

Goal: Convert token IDs into continuous vector representations.

We'll code:

Token Embedding

Sinusoidal Positional Encoding

â¡ï¸ Output: tensor of shape (batch_size, seq_len, d_model)

âš¡ 2. Scaled Dot-Product Attention

Goal: Compute attention weights between tokens.

We'll code:

Queries (Q), Keys (K), Values (V)

Attention formula

### ğŸ§  Scaled Dot-Product Attention

The core operation behind the Transformer is the **Scaled Dot-Product Attention**, defined as:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^{T}}{\sqrt{d_k}}\right)V
$$

Where:
- \( Q \) = Query matrix  
- \( K \) = Key matrix  
- \( V \) = Value matrix  
- \( d_k \) = dimensionality of the key vectors


â¡ï¸ Output: context vectors (weighted representations)

ğŸ§© 3. Multi-Head Attention

Goal: Run multiple attention heads in parallel.

We'll code:

Linear projections for multiple heads

Head splitting and concatenation

Output projection layer

â¡ï¸ Output: richer contextual embeddings

ğŸ” 4. Feed Forward Network (FFN)

Goal: Add non-linear transformations per token.

We'll code:

Linear â†’ GELU â†’ Linear

Dropout (optional)

â¡ï¸ Output: transformed representation per position

ğŸ§  5. Layer Normalization + Residual Connections

Goal: Stabilize and accelerate training.

We'll code:

LayerNorm(x + Sublayer(x))

Residual skips between attention and feed-forward layers

ğŸ§° 6. Encoder Layer

Goal: Stack multiple layers of attention + FFN.

We'll code:

Self-Attention + FFN + Norm + Residual

N identical layers in sequence

ğŸ’¬ 7. Decoder Layer

Goal: Generate sequences autoregressively.

We'll code:

Masked Multi-Head Self-Attention

Encoderâ€“Decoder Attention

Feed Forward Network

Norm + Residual

ğŸ§© 8. Transformer (Full Model)

Goal: Combine encoder + decoder into one model.

We'll code:

Encoder â†’ Decoder â†’ Linear Projection â†’ Softmax

â¡ï¸ Output: logits over target vocabulary.

ğŸ”¬ 9. Training Loop (Toy Task)

Goal: See your model learn something.

We'll code:

Forward pass

Cross-entropy loss

Optimization step

Evaluate toy copy/translate task


---

ğŸ§  Concepts Youâ€™ll Learn

    Linear Algebra in neural networks (matrix operations)
    
    Self-Attention and Multi-Head Attention
    
    Positional Encoding and Sequence Order
    
    Encoderâ€“Decoder architecture
    
    Layer Normalization and Residual Connections
    
    How Transformers learn sequence relationships

---

ğŸ“˜ Educational Goals

    ScratchFormer was built to:
    
    Teach the core principles of Transformers
    
    Provide a readable, minimal implementation for learning
    
    Serve as a foundation for experiments or PyTorch conversion
    
    Help developers, students, and AI enthusiasts understand every line of a Transformer

---

ğŸ“š References

    Attention Is All You Need (Vaswani et al., 2017)
     url-> https://arxiv.org/abs/1706.03762
    
    The Illustrated Transformer â€” Jay Alammar
     url-> https://jalammar.github.io/illustrated-transformer/
    
    3Blue1Brown â€” Linear Algebra Series**
     url-> https://www.3blue1brown.com/topics/linear-algebra
      


---

ğŸ§° Requirements

    Python 3.9+
    NumPy 1.26+

---

ğŸª„ Future Enhancements

    PyTorch version for training with autograd
    
    Visualization of attention maps
    
    GPT-style text generation
    
    Colab notebook for interactive learning

---

ğŸ“„ License

    MIT License Â© 2025 Vinit Parmar
