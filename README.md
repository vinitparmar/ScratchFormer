# ğŸ§© ScratchFormer

**ScratchFormer** is a Transformer neural network built **entirely from scratch** using **NumPy**.  
This project helps you deeply understand how modern architectures like GPT and BERT actually work â€” by implementing every part manually, from attention to embeddings.

---

## ğŸš€ Features
- âœ… Pure **NumPy** implementation (no deep learning frameworks)
- ğŸ§  Full **Encoderâ€“Decoder Transformer** architecture
- ğŸ” Implements:
  - Scaled Dot-Product & Multi-Head Attention  
  - Positional Encoding (sinusoidal)  
  - Feed Forward Networks (GELU activation)  
  - Layer Normalization & Residual Connections
- ğŸ§© Modular and easy to extend
- ğŸ’¡ Educational, readable, and well-documented
- ğŸ§ª Includes toy training example (copy / translation task)

---

## ğŸ—ï¸ Project Structure

ScratchFormer/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ scratchformer/
â”‚ â”œâ”€â”€ init.py
â”‚ â””â”€â”€ transformer_from_scratch_numpy.py # Main transformer implementation
â”‚
â”œâ”€â”€ examples/
â”‚ â””â”€â”€ copy_task_demo.py # Simple demo/training example
â”‚
â””â”€â”€ notebooks/
â””â”€â”€ transformer_from_scratch.ipynb # (Optional) Jupyter notebook for explanation


ğŸ§  Concepts Youâ€™ll Learn

Linear Algebra in neural networks (matrix operations)

Self-Attention and Multi-Head Attention

Positional Encoding and Sequence Order

Encoderâ€“Decoder architecture

Layer Normalization and Residual Connections

How Transformers learn sequence relationships

---

ğŸ“˜ Educational Goals

ScratchFormer was built to:

Teach the core principles of Transformers

Provide a readable, minimal implementation for learning

Serve as a foundation for experiments or PyTorch conversion

Help developers, students, and AI enthusiasts understand every line of a Transformer

---

ğŸ“š References

Attention Is All You Need (Vaswani et al., 2017)

The Illustrated Transformer â€” Jay Alammar

3Blue1Brown â€” Linear Algebra Series


---

ğŸ§° Requirements

Python 3.9+
NumPy 1.26+

---

ğŸª„ Future Enhancements

PyTorch version for training with autograd

Visualization of attention maps

GPT-style text generation

Colab notebook for interactive learning

---

ğŸ“„ License

MIT License Â© 2025 Vinit Parmar
